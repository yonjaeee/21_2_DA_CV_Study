## Ch.11 RetinaNet / EfficientDet



### RetinaNet

![1](https://user-images.githubusercontent.com/77441568/144402767-9c756e11-e686-46f6-b55a-e2768802784e.png)

- 2017년 Facebook AI Research team에서 발표
- One Stage Detector의 성능 저하 문제 개선
- 수행시간: Faster RCNN > RetinaNet > Yolo, SSD

- 작은 object에 대해 성능이 좋음
- **Focal Loss + Feature Pyramid Network**
- Top ranking -> RetinaNet을 사용한 경우가 많음



#### Focal Loss

<img src="https://user-images.githubusercontent.com/77441568/144402852-d6fc5f27-dbfe-4427-82da-047ca5564e27.png" alt="2" style="zoom:67%;" />

- Cross Entropy의 변형

- 잘하는 애들 더 잘하게, 못하는 애들 덜 못하게
- Cross Entropy에 가중치 부여(동적으로 Cross entropy 조절)

![5](https://user-images.githubusercontent.com/77441568/144402914-39d72799-c0f8-4aac-a6e9-678da4e57245.png)

- 확실한 Object들에 매우 작은 loss 부여

![6](https://user-images.githubusercontent.com/77441568/144402973-749dc3f9-2746-4afa-bfa7-a002f2de9636.png)



#### One Stage Object Detector - Class imbalance issue

![3](https://user-images.githubusercontent.com/77441568/144403027-5ae18209-1ce6-439a-a956-b63bf457d572.png)

- background(negative) example이 대부분
- 많은 anchor box -> background에 치중
- 매우 적은 foreground(positive) example (유용한 정보 제공)
- **Easy Example**
  - 찾기 쉬운 대상들
  - Background
  - 선명한 object
  - 이미 높은 예측 확률
- **Hard Example**
  - 찾기 어려운 대상들
  - 작고 형태가 불분명
  - 낮은 예측 확률
- Easy Example 수 >>> Hard Example 수
- Region Proposal, Detection 동시 수행 -> Easy Example에 학습이 치우침(성능 저하)

![4](https://user-images.githubusercontent.com/77441568/144403068-a1ec59ba-5952-4a62-83e6-4f33d19827e3.png)

#### FPN(Feature Pyramid Network)

![7](https://user-images.githubusercontent.com/77441568/144403108-6970b0ec-3747-43fb-a1f5-7190bab46158.png)

![8](https://user-images.githubusercontent.com/77441568/144403149-733a791a-763f-4018-8be4-be5866a69578.png)

- bottom up / top down 방식으로 추출된 feature map들을 lateral connection으로 연결하는 방식

- resolution이 떨어지는 문제 ->  skip connection으로 보완
- upsampling을 하되, skip connection으로 정보를 가져옴
- 3x3: 2개가 섞이며 서로의 특성을 잃어버리는 것 방지

![9](https://user-images.githubusercontent.com/77441568/144403181-ed7b408c-22f0-483a-a780-b7bfb428b7a2.png)



### EfficientDet

![10](https://user-images.githubusercontent.com/77441568/144403241-a2d3b961-2473-47e7-b8d8-778fa46429ec.png)

- Scalable and Efficient Object Detection
- backbone: EfficientNet
- FPN -> BiFPN 변경(FPN의 성공으로 발전시키려는 노력 많았음)
- Compound Scaling



#### 성능

![11](https://user-images.githubusercontent.com/77441568/144403286-8d120836-46fa-42c7-88fc-89d8eaeaca90.png)

- 적은 연산 수, 적은 parameter 수
- 높은 모델 예측 성능
- original yolo-v3 보다 inference 속도가 조금 더 빠름



#### BiFPN (Bi directional FPN)

- Feature Fusion
- FPN -> PANet -> NAS-FPN -> BiFPN
- FPN에 비해서 많은 input Feature map
- Weighted Feature Fusion (가중치를 다르게 주어 Feature Fusion)



##### Cross Scale Connections

![12](https://user-images.githubusercontent.com/77441568/144403331-f8c15021-612d-473c-a765-36322fb3db19.png)

- FPN의 최상단 -> 추상화가 많이 됨, 큰 Obj

- PANet : (높은 추상화 -> 낮은 추상화) + (낮은 추상화 -> 높은 추상화), 큰 가능성 발견

- NAS-FPN: 강화학습으로 최적 Network 찾음

- BiFPN: PANet과 거의 같음, 처음/마지막 feature는 합치는 의미 x -> 삭제



##### Weighted Feature Fusion

- 서로 다른 resolution(feature map size)를 가지는 input feature map들은 output feature map을 생성하는 기여도가 다름
- 서로 다른 가중치를 부여하여 fusion
- weight -> 학습을 통해 도출

![13](https://user-images.githubusercontent.com/77441568/144403379-1a1ae1b9-c6e2-4af9-94fd-0b13c4e1acdf.png)

- boundary를 정하기 위해 softmax-based fusion 적용 -> 너무 느림

- Fast normalized fusion 적용



![14](https://user-images.githubusercontent.com/77441568/144403429-0d6ec977-ff05-42af-b220-876ae1dd984a.png)

- 연산량 감소를 위해 Separable Convolution 적용



#### EfficientNet - Backbone

- Depth(Network), Width(필터 수), Resolution 크기를 함께 최적으로 조합하여 성능 극대화



![15](https://user-images.githubusercontent.com/77441568/144403472-e9dd4cc9-5021-4cdf-bf7f-47da919de6ba.png)

- 필터 수, 깊이를 일정 수준 이상 늘려도 성능 향상이 잘 되지 않음



##### Compound Scaling

![16](https://user-images.githubusercontent.com/77441568/144403517-4bfdc122-1167-4d11-98ee-e336055cc266.png)

- 개별 Scaling 요소를 증가시키더라도 성능 향상이 어려움
- 3가지 Scaling Factor를 동시에 고려하는 Compound Scaling 적용
- 최적 Scaling 도출 기반 식을 통해 찾음
- B0 ~ B7 사이 중 각 용도에 맞게 사용

![17](https://user-images.githubusercontent.com/77441568/144403557-0c96c7ef-225e-4f97-b914-9b76e783561c.png)



#### EfficientDet Compoung Scaling

- 거대한 Backbone, 여러 층의 FPN, 큰 input image의 크기 등의 개별적인 부분에 집중하는 것은 비효율적
- EfficientNet과 같이 Backbone, BiFPN, Prediction layer, input image size를 scaling 기반으로 최적 결합
- D0~D7 모델 구성



##### Backbone network

- EfficientNet B0-B6로 Scaling 그대로 적용



##### BiFPN network

$$
D_{bifpn} = 3 + \varnothing
$$



- 기본 반복 block 3개로 설정, Scaling 적용

$$
W_{bifpn} = 64*(1.35^\varnothing)
$$



- width는 Grid Search를 통해 1.35로 선택



##### Prediction  Network(Head)

- width는 BiFPN 채널 수와 동일
- Depth

$$
D_{box} = D_{class} = 3 + [\varnothing/3]
$$

##### input image size

$$
R_{input} = 512 + \varnothing*128
$$

![18](https://user-images.githubusercontent.com/77441568/144403593-d5ac5ec9-141e-4400-95c0-1ee69266465d.png)

![19](https://user-images.githubusercontent.com/77441568/144403618-b0205294-8bf0-4c05-86e2-4b54e5116c54.png)

